---
title: "Introduction to supervised changepoint detection"
author: Toby Dylan Hocking, McGill University, Montreal, Canada
output: 
  html_document:
    toc: true
    toc_depth: 2
---

# What is the difference between unsupervised and supervised changepoint detection?

## Neuroblastoma data set

We will use the neuroblastoma data set to explore supervised
changepoint detection.

```{r neuroblastoma}
options(width=100)
data(neuroblastoma, package="neuroblastoma")
str(neuroblastoma)
lapply(neuroblastoma, head, 10)
```

The neuroblastoma data set consists of two data.frames:

* profiles contains the noisy data (logratio), which is approximate
  DNA copy number (measured at chromosome, position), for a particular
  patient (profile.id). Every unique combination of (profile.id,
  chromosome) defines a separate multiple changepoint detection
  problem. The logratio measures how many copies of each part of the
  human genome are present in the patient's tumor. Normally there are
  two copies of each chromosome (logratio=0), but in cancer samples
  there can be gains and losses of specific regions or entire
  chromosomes.
* annotations contains the labels, which indicate presence
  (breakpoint) or absence (normal) of changepoints in specific regions
  (profile.id, chromosome, min, max) of the data.
  
These data come from children who were treated at the Institut Curie
(Paris, France). For six children we also have follow-up data on
whether they recovered (ok) or had a relapse, several years after
treatment:

```{r clinical}
followup <- data.frame(
  profile.id=c(10L, 8L, 4L, 6L, 11L, 1L),
  status=c("ok", "relapse", "relapse", "ok", "ok", "relapse"))
followup
```
  
## Unsupervised: no labels, model selection via theory

When there are no labels, the input for a changepoint analysis just
uses the noisy DNA copy number data in
`neuroblastoma$profiles$logratio`. We begin by selecting the profiles
of the six patients for which we have follow-up data.

```{r six}
rownames(followup) <- followup$profile.id
followup$status.profile <- with(followup, paste(status, profile.id))
some.ids <- rownames(followup)
library(data.table)
someProfiles <- function(all.profiles){
  some <- subset(all.profiles, profile.id %in% some.ids)
  status.profile <- followup[paste(some$profile.id), "status.profile"]
  some$status.profile <- ifelse(
    is.na(status.profile), some$profile.id, status.profile)
  data.table(some)
}
six.profiles <- someProfiles(neuroblastoma$profiles)
```

For these six patients, the total number of separate changepoint
detection problems is 24 chromosomes x 6 patients = 144 problems. We
plot each problem below in a separate facet (panel). 

```{r plotProfiles, fig.width=10}
library(ggplot2)
gg.unsupervised <- ggplot()+
  ggtitle("unsupervised changepoint detection = only noisy data sequences")+
  theme(
    panel.margin=grid::unit(0, "lines"),
    panel.border=element_rect(fill=NA, color="grey50")
  )+
  facet_grid(status.profile ~ chromosome, scales="free", space="free_x")+
  geom_point(aes(position/1e6, logratio),
             data=six.profiles,
             shape=1)+
  scale_x_continuous(
    "position on chromosome (mega bases)",
    breaks=c(100, 200))+
  scale_y_continuous(
    "logratio (approximate DNA copy number)",
    limits=c(-1,1)*1.1)
print(gg.unsupervised)
```

Note the facet titles on the right:

* The top three profiles are "ok" because those children
  completely recovered.
* The bottom three profiles are "relapse" because the
  neuroblastoma cancer came back, and required another treatment at
  the hospital.

**Question:** can you visually identify any differences between the
"ok" and "relapse" profiles?

How to choose the number of changepoints? In the unsupervised setting,
our only option is to use theoretical/statistical arguments, which
give information criteria such as AIC (penalty=2) or BIC/SIC
(penalty=log n). More on this later.

## Supervised: learn a penalty function with minimal incorrect labels

In supervised changepoint detection, there are labels which indicate
presence and absence of changepoints in particular data subsets. For
the same set of 6 profiles, we superimpose the labels in the plot
below.

```{r plotLabels, fig.width=10}
six.labels <- someProfiles(neuroblastoma$annotations)
gg.supervised <- gg.unsupervised+
  ggtitle(paste(
    "supervised changepoint detection = data + labels",
    "that indicate specific regions with or without changepoints"))+
  penaltyLearning::geom_tallrect(aes(
    xmin=min/1e6, xmax=max/1e6, fill=annotation),
    alpha=0.5,
    color=NA,
    data=six.labels)+
  scale_fill_manual("label", values=penaltyLearning::change.colors)+
  theme(legend.position="bottom")
print(gg.supervised)
```

It is clear from this plot that the neuroblastoma data satisfy the two
criteria which are necessary for a supervised changepoint analysis:

* The data consist of **several sequences** with similar signal/noise
  patterns, which are treated as separate changepoint detection
  problems.
* Some data sequences have **labels** which indicate specific regions
  where the model should predict presence or absence of changepoints.
  
As we will see later in the tutorial, the labels can be used for
choosing the best model and parameters. The main idea is that the
changepoint model should be consistent with the labels:

* Every breakpoint/positive label is a region where the model should
  predict at least one changepoint. If the model predicts no
  changepoints in a region with a breakpoint label, that is considered
  a false negative.
* Every normal/negative label is a region where the model should
  predict no changepoints. If the model predicts one or more
  changepoints in a region with a normal label, that is considered a
  false positive.
* The overall goal is find a changepoint model that minimizes the
  number of incorrectly predicted labels, which is defined as the sum
  of false positives and false negatives.

**Exercise:** plot data and labels for a different set of
profiles. Hint: assign a new value to `some.ids`, then re-run the code
above.

## Creating labels via visual inspection

Labels can be created using prior knowledge or visual inspection. In
this section we explain how to create labels via visual inspection.

**Exercise:** create a set of labels via visual inspection for one
un-labeled data sequence in the neuroblastoma data set. Begin by
plotting one data set by itself to see the details of the
signal and noise

```{r zoom}
zoom.profile <- 4 #Exercise: change this value!
zoom.chromosome <- 14 #Exercise: change this value!
zoom.pro <- subset(
  neuroblastoma$profiles,
  profile.id==zoom.profile & chromosome==zoom.chromosome)
zoom.gg <- ggplot()+
  geom_point(aes(position/1e6, logratio),
             data=zoom.pro,
             shape=1)+
  scale_y_continuous(
    "logratio (approximate DNA copy number)")
print(zoom.gg)
```

Then add some new labels by creating a `data.frame` in R that encodes
where you saw regions with and without changepoints. Hints:

* in the neuroblastoma data set there is only one label per data
  sequence, but you can add as many as you want. Try adding two labels
  to the same data sequence.
* in addition to breakpoint/normal labels, you can use the "1change"
  label to indicate a region that should have exactly 1 changepoint (2
  or more implies a false positive, 0 implies a false negative).
* it will simplify the code to use a helper function as below:

```{r manual-label-fun}
label <- function(min, max, annotation){
  data.frame(
    profile.id=zoom.profile,
    chromosome=zoom.chromosome,
    min, max, annotation)
}
zoom.labels <- rbind(
  label(70e6, 80e6, "1change"),
  label(20e6, 60e6, "normal"))
zoom.gg.lab <- zoom.gg+
  penaltyLearning::geom_tallrect(aes(
    xmin=min/1e6, xmax=max/1e6, fill=annotation),
    alpha=0.5,
    color=NA,
    data=zoom.labels)+
  scale_fill_manual("label", values=penaltyLearning::change.colors)
print(zoom.gg.lab)
```

To simplify the process of labeling via visual inspection, I recommend using
a GUI such as
[annotate_regions.py](https://pypi.python.org/pypi/annotate_regions/1.0)
or [SegAnnDB](https://github.com/tdhock/SegAnnDB).

# A typical supervised changepoint analysis

In this section we explain how to perform a supervised changepoint
analysis on a labeled data set.

## Overview of supervised changepoint computations

A typical supervised changepoint analysis consists of the
following computations:

* [Changepoints and model selection](#changepoints-and-model-selection).
  For each of several labeled segmentation problems (data sequences
  that are separate but have a similar signal/noise pattern), use your
  favorite changepoint detection package to compute a sequence of
  models of increasing complexity (say from 0 to 20 changepoints). For
  each segmentation problem, make sure to save the changepoint
  positions of each model, and use `modelSelection` to compute the
  exact path of models that will be selected for every possible
  non-negative penalty value.
* [Label error](#label-error). Use `labelError` to compute the number
  of incorrect labels for each labeled segmentation problem and each
  changepoint model. The goal of learning is to minimize the number of
  incorrectly predicted labels.
* [Outputs](#outputs). Use `targetIntervals` to compute a target
  interval of log(penalty) values that predicts the minimum number of
  incorrect labels for each segmentation problem. Create a target
  interval matrix (one row for each segmentation problems, 2 columns)
  which can be used as the output (target.mat argument) in the
  `IntervalRegression*` functions.
* [Inputs](#inputs). Compute a feature matrix (segmentation problems x
  features) using `featureMatrix`. Features can be
  simple statistics of each segmentation problem (quantiles, mean,
  number of data points, estimated variance, etc).
* [Learning](#learning). Use `IntervalRegressionCV` to learn a
  penalty function, and automatically perform feature selection using
  L1 regularization. If you want to evaluate the prediction accuracy
  of the model you learn, make sure to set aside a "test set" of
  labeled data that you do not use in this learning step (this method is
  called cross-validation).
* [Prediction](#prediction). Use the `predict.IntervalRegression`
  method to compute predicted penalty values for all the segmentation
  problems. This works even for unlabeled data, and for "test" data
  sets where you have hidden some labels in a computational
  cross-validation experiment.
* [Evaluation](#evaluation). If you have set aside a "test set" of
  labels that you did not use for learning, then you can now use
  `ROChange` to compute test ROC curves. The AUC (area under the ROC
  curve) and the percent incorrect labels in the test set can be used
  to evaluate the prediction accuracy of your model.

## Changepoints and model selection

**Exercise** for the data sequence that you labeled above, compute
optimal Gaussian changepoint models for 1 to 10 segments using the
code below.

```{r}
max.segments <- 10
(fit <- Segmentor3IsBack::Segmentor(
  zoom.pro$logratio, model=2, Kmax=max.segments))
```

The `fit` object is an S4 class with slots

* `breaks` the end position of each segment.
* `parameters` the mean of each segment.

Next, we convert the model to tidy format (a `data.frame` with one row
per segment). The code below is specific to the `Segmentor` function;
if you use a different changepoint detection package/function, you
will have to write some other code to tidy the data.

```{r}
zoom.segs.list <- list()
zoom.loss.vec <- rep(NA, max.segments)
for(n.segments in 1:max.segments){
  end <- fit@breaks[n.segments, 1:n.segments]
  data.before.change <- end[-n.segments]
  data.after.change <- data.before.change+1
  pos.before.change <- as.integer(
  (zoom.pro$position[data.before.change]+
   zoom.pro$position[data.after.change])/2)
  start <- c(1, data.after.change)
  chromStart <- c(zoom.pro$position[1], pos.before.change)
  chromEnd <- c(pos.before.change, max(zoom.pro$position))
  seg.mean.vec <- fit@parameters[n.segments, 1:n.segments]
  zoom.segs.list[[n.segments]] <- data.frame(
    profile.id=zoom.profile,
    chromosome=zoom.chromosome,
    n.segments,
    start,
    end,
    chromStart,
    chromEnd,
    mean=seg.mean.vec,
    row.names=NULL)
  data.mean.vec <- rep(seg.mean.vec, end-start+1)
  stopifnot(length(data.mean.vec)==nrow(zoom.pro))
  zoom.loss.vec[n.segments] <- sum((zoom.pro$logratio-data.mean.vec)^2)
}
```

Notice that we computed `zoom.loss.vec` which is the sum of squared
residuals of each model. Below we use that to construct a data.frame with
one row for each model, which we can use with `modelSelection` to
compute the models that will be selected for every possible penalty
value. 

```{r}
zoom.models <- data.frame(
  profile.id=zoom.profile,
  chromosome=zoom.chromosome,
  loss=zoom.loss.vec, n.segments=as.numeric(1:max.segments))
(zoom.selection <- penaltyLearning::modelSelection(
  zoom.models, complexity="n.segments"))
```

If $\lambda\geq 0$ is the non-negative penalty value, and $L_{i,s}$ 
is the loss of the model with $s\in\{1,\dots,s_{\text{max}}\}$
segments for data sequence $i$, then the model selection function is

$$ s_i^*(\lambda) = \text{arg min}_s L_{i,s} + \lambda s $$
	
```{r}
ggplot()+
  geom_segment(aes(
    min.lambda, n.segments,
    xend=max.lambda, yend=n.segments),
    size=1,
    data=zoom.selection)+
  xlab("penalty constant = lambda")+
  scale_y_continuous(breaks=zoom.selection$n.segments)
ggplot()+
  geom_segment(aes(
    min.log.lambda, n.segments,
    xend=max.log.lambda, yend=n.segments),
    size=1,
    data=zoom.selection)+
  xlab("log(penalty) = log(lambda)")+
  scale_y_continuous(breaks=zoom.selection$n.segments)
```

As can be seen from the definition of $s_i^*(\lambda)$ and the figures
above, a bigger penalty $\lambda$ results in a model with fewer segments/changes (and vice versa). Note that there are some models that are not selected for
any penalty values (e.g. 3 or 6 segments for this data set).

The predicted changepoint positions are every `chromStart` on segments
after the first:

```{r}
(zoom.segs <- do.call(rbind, zoom.segs.list[zoom.selection$n.segments]))
(zoom.changes <- subset(zoom.segs, 1 < start))
```

We use the code below to visualize the models along with the noisy
data sequence:

```{r}
zoom.gg.models <- zoom.gg.lab+
  theme(
    panel.margin=grid::unit(0, "lines"),
    panel.border=element_rect(fill=NA, color="grey50")
  )+
  facet_grid(n.segments ~ .)+
  geom_vline(aes(
    xintercept=chromStart/1e6),
    data=zoom.changes,
    color="green",
    size=1,
    linetype="dashed")+
  geom_segment(aes(
    chromStart/1e6, mean,
    xend=chromEnd/1e6, yend=mean),
    data=zoom.segs,
    size=1,
    color="green")
print(zoom.gg.models)
```

## Label error

Next, we compute the number of incorrect labels for each model. The
`labelError` function inputs 3 data.frames:

* `models` one row per changepoint model, with columns for loss and
  model complexity.
* `labels` one row for each label, with columns for label location and
  type.
* `changes` one row for each predicted changepoint, with a column for
  the predicted changepoint position.

```{r}
zoom.error.list <- penaltyLearning::labelError(
  zoom.selection,
  zoom.labels,
  zoom.changes,
  problem.vars=c("profile.id", "chromosome"))
str(zoom.error.list)
```

The result of `labelError` is a list of two data tables:

* `model.errors` has one row per (sequence,model), with the total number
  of incorrect labels.
* `label.errors` has one row per (sequence,model,label), with the
  fp/fn status of each label.
  
**Exercise** visualize the false negatives and false positives in the
data set that you labeled, using the code below.

```{r}
zoom.gg.models+
  penaltyLearning::geom_tallrect(aes(
    xmin=min/1e6,
    xmax=max/1e6,
    linetype=status),
    data=zoom.error.list$label.errors,
    fill=NA)+
    scale_linetype_manual("error type", values=c(
      correct=0,
      "false negative"=3,
      "false positive"=1))
```

* **False positives** are labels with too many predicted changepoints,
  drawn with a solid black border.
* **False negatives** are labels with too few predicted changepoints,
  drawn with a dotted black border.
* **Question** which models are consistent with the labels in your data?

## Outputs

Now that we have computed the number of incorrect labels
$e_i(s)\in\{0,1,2,\dots\}$ for this data sequence $i$ and every number of
segments $s$, we can visualize the label error as function of the
penalty, $$ E_i(\lambda) = e_i[s^*_i(\lambda)] $$

```{r}
zoom.errors.tall <- data.table::melt(
  zoom.error.list$model.errors,
  measure.vars=c("n.segments", "errors"))
zoom.gg.errors <- ggplot()+
  geom_segment(aes(
    min.log.lambda, value,
    xend=max.log.lambda, yend=value),
    size=1,
    data=zoom.errors.tall)+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  facet_grid(variable ~ ., scales="free")+
  scale_y_continuous("", breaks=0:max.segments)+
  xlab("log(penalty) = log(lambda)")
print(zoom.gg.errors)
```

Below we compute the target interval of penalty values that select
changepoint models with minimal incorrect labels.
  
```{r}
zoom.target <- penaltyLearning::targetIntervals(
  zoom.error.list$model.errors,
  problem.vars=c("profile.id", "chromosome"))
zoom.target.tall <- data.table::melt(
  zoom.target,
  measure.vars=c("min.log.lambda", "max.log.lambda"),
  variable.name="limit")[is.finite(value)]
zoom.gg.errors+
  geom_point(aes(
    value,
    errors,
    fill=limit),
    shape=21,
    size=4,
    data=data.frame(zoom.target.tall, variable="errors"))+
  scale_fill_manual("limit", values=c(
    min.log.lambda="black",
    max.log.lambda="white"))  
```

The plot above emphasizes the target interval, which is used as the
output in the machine learning problem:

* the black dot shows the lower limit of the target interval. If a
  smaller log(penalty) is predicted, then there are too many changepoints (false
  positives).
* the white dot shows the upper limit of the target interval. If a
  larger log(penalty) is predicted, then there are too few
  changepoints (false negatives).
  
The target interval is different for each labeled data sequence. Below
we compute the target interval for each of the labeled data sequences
in the six profiles that we showed in the beginning of the tutorial.
  
```{r}
six.problems.dt <- unique(six.profiles[, list(profile.id, chromosome, status.profile)])
setkey(six.profiles, profile.id, chromosome)
six.segs.list <- list()
six.selection.list <- list()
for(problem.i in 1:nrow(six.problems.dt)){
  meta <- six.problems.dt[problem.i,]
  pro <- six.profiles[meta]
  max.segments <- min(nrow(pro), 10)
  fit <- Segmentor3IsBack::Segmentor(pro$logratio, model=2, Kmax=max.segments)
  rss.vec <- rep(NA, max.segments)
  for(n.segments in 1:max.segments){
    end <- fit@breaks[n.segments, 1:n.segments]
    data.before.change <- end[-n.segments]
    data.after.change <- data.before.change+1
    pos.before.change <- as.integer(
      (pro$position[data.before.change]+pro$position[data.after.change])/2)
    start <- c(1, data.after.change)
    chromStart <- c(pro$position[1], pos.before.change)
    chromEnd <- c(pos.before.change, max(pro$position))
    seg.mean.vec <- fit@parameters[n.segments, 1:n.segments]
    data.mean.vec <- rep(seg.mean.vec, end-start+1)
    rss.vec[n.segments] <- sum((pro$logratio-data.mean.vec)^2)
    six.segs.list[[paste(problem.i, n.segments)]] <- data.table(
      meta,
      n.segments,
      start,
      end,
      chromStart,
      chromEnd,
      mean=seg.mean.vec)
  }
  loss.dt <- data.table(
    meta,
    n.segments=1:max.segments,
    loss=rss.vec)
  six.selection.list[[problem.i]] <- penaltyLearning::modelSelection(
    loss.dt, complexity="n.segments")
}

six.selection <- do.call(rbind, six.selection.list)
six.segs <- do.call(rbind, six.segs.list)
six.changes <- six.segs[1 < start]
six.error.list <- penaltyLearning::labelError(
  six.selection, six.labels, six.changes,
  problem.vars=c("profile.id", "chromosome"))
six.targets <- penaltyLearning::targetIntervals(
  six.error.list$model.errors,
  problem.vars=c("profile.id", "chromosome"))
six.targets.tall <- data.table::melt(
  six.targets,
  measure.vars=c("min.log.lambda", "max.log.lambda"),
  variable.name="limit",
  value.name="log.lambda")[is.finite(log.lambda)]
ggplot()+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  facet_grid(profile.id ~ chromosome)+
  geom_segment(aes(
    min.log.lambda, errors,
    xend=max.log.lambda, yend=errors),
    size=1,
    data=six.error.list$model.errors)+
  geom_point(aes(
    log.lambda,
    errors,
    fill=limit),
    shape=21,
    size=4,
    data=six.targets.tall)+
  scale_fill_manual("limit", values=c(
    min.log.lambda="black",
    max.log.lambda="white"))+
  scale_y_continuous(
    "incorrectly predicted labels",
    limits=c(0,1.2),
    breaks=c(0,1))+
  scale_x_continuous(
    "log(penalty) = log(lambda)")
```

It is clear from the plot above that a model with minimal incorrect
labels will predict a different penalty for each data sequence. 

**Question:** discuss with your neighbor for 1 minute, why is there
only one limit per data sequence? (only one target limit point per
panel in the plot above)

## Inputs

To train a machine learning model, we first need to compute a vector
$x_i\in\mathbb R^p$ of exactly $p$ features for each data sequence
$i$. We then learn a function $f(x_i)=\log\lambda_i\in\mathbb R$. 

**Example:** The BIC/SIC model selection criterion is $\lambda_i =
\log n_i$, where $n_i$ is the number of data points to segment in
sequence $i$. Thus in this framework the BIC/SIC criterion is a linear
model with the feature $x_i=\log\log n_i$.  
$$f(x_i) = \log\lambda_i = x_i = \log\log n_i$$

The plot below visualizes the target intervals in this feature space.

```{r}
six.BIC.dt <- six.profiles[, list(
  log.log.n=log(log(.N))
), by=list(profile.id, chromosome)]
six.features.tall <- six.BIC.dt[six.targets.tall, on=list(
  profile.id, chromosome)]
six.gg.limits <- ggplot()+
  geom_point(aes(
    log.log.n, log.lambda, fill=limit),
    shape=21,
    data=six.features.tall)+
  scale_fill_manual("limit", values=c(
    min.log.lambda="black",
    max.log.lambda="white"))+
  scale_x_continuous(
    "feature log(log(n))")+
  scale_y_continuous(
    "log(penalty)=log(lambda)")
print(six.gg.limits)
```

In the plot above, each point represents a finite limit of a target
interval. The goal of learning is to find a function which is below
the white dots (upper limits) and above the black dots (lower
limits). Note that this is not the same problem as binary
classification, which the plot below makes clear.

```{r}
six.features.targets <- six.BIC.dt[six.targets, on=list(
  profile.id, chromosome)]
six.gg.limits+
  geom_segment(aes(
    log.log.n, min.log.lambda,
    xend=log.log.n, yend=max.log.lambda),
    data=six.features.targets)
```

In the plot above, each line segment represents the target interval of
penalty values that achieves minimal incorrect regions for a labeled
data sequence. The goal of learning is to find a function that
intersects as many intervals as possible. 

In this context we can plot the BIC/SIC model as a line with slope 1
and intercept 0, and we can visualize the errors it makes using the
red line segments below.

```{r}
six.features.targets[, pred.log.lambda := log.log.n ]
six.features.targets[, residual := penaltyLearning::targetIntervalResidual(
  cbind(min.log.lambda, max.log.lambda), pred.log.lambda)]
six.gg.limits+
  geom_abline(slope=1, intercept=0)+
  geom_segment(aes(
    log.log.n, pred.log.lambda,
    xend=log.log.n, yend=pred.log.lambda-residual),
    color="red",
    data=six.features.targets)
```

We can see from the plot above that there are 5 data sequences for
which the BIC predicts a penalty value outside of the target
interval. We can visualize the number of incorrect labels for this
model by plotting the data and labels below.

```{r}
six.BIC.selection <- data.table(six.selection)[six.BIC.dt, on=list(
  profile.id, chromosome,
  min.log.lambda < log.log.n,
  max.log.lambda > log.log.n)]
six.BIC.labels <- six.error.list$label.errors[six.BIC.selection, on=list(
  profile.id, chromosome, n.segments), nomatch=0L]
six.BIC.changes <- six.changes[six.BIC.selection, on=list(
  profile.id, chromosome, n.segments), nomatch=0L]
six.BIC.segs <- six.segs[six.BIC.selection, on=list(
  profile.id, chromosome, n.segments)]
gg.supervised+
  ggtitle("BIC/SIC model changepoints and label errors")+
  penaltyLearning::geom_tallrect(aes(
    xmin=min/1e6,
    xmax=max/1e6,
    linetype=status),
    fill=NA,
    data=six.BIC.labels)+
  scale_linetype_manual("error type", values=c(
    correct=0,
    "false negative"=3,
    "false positive"=1))+
  geom_vline(aes(
    xintercept=chromStart/1e6),
    data=six.BIC.changes,
    color="green",
    size=1,
    linetype="dashed")+
  geom_segment(aes(
    chromStart/1e6, mean,
    xend=chromEnd/1e6, yend=mean),
    data=six.BIC.segs,
    size=1,
    color="green")+
  theme(legend.box="horizontal")
```


**Question:** what is the feature $x_i$ 

* Compute a feature vector for each segmentation problem, and a
  feature matrix for each labeled set of related segmentation
  problems.
  
## Learning

* Learn an affine function f(feature vector)=log(penalty).
* Exercise: to learn the coefficients of the BIC penalty, what feature
  vector should be used?
* Un-regularized interval regression (survival package). Learns
  weights for a given set of features, but may overfit if
  non-relevant features are used.
* L1-regularized interval regression. Simultaneously learns weights
  and performs feature selection. Avoids overfitting by setting some
  feature weights to zero.

## Prediction

## Evaluation

* K-fold cross-validation can be used to compare prediction accuracy
  of supervised and unsupervised changepoint detection.
* Compute test error and ROC curves for unsupervised and supervised
  penalty functions: BIC, 1 feature un-regularized, multi-feature
  un-regularized, multi-feature regularized. Which penalty function is
  most accurate?
* Exercise: perform cross-validation to compare Gaussian and Logistic
  models for log(penalty) values. Which distribution results in more
  accurate penalty functions?
 
